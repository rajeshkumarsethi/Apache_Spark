# -*- coding: utf-8 -*-
"""Spark_python_dataframes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YimmJ1M8WSCdZOR1rAuUA2Y3SF_1MKWS
"""

!pip install pyspark

from pyspark.sql import SparkSession

# Creating an spark environment 
# Name of the spark session is SparkExamples (User defined)
spark = SparkSession.builder.appName("SparkExamples").getOrCreate()

dataList = [("Java", 20000), ("Python", 100000), ("Scala", 3000)]

# RDD: Resilient Distributed Dataset
rdd=spark.sparkContext.parallelize(dataList)

rdd.count()

# Collect is like View. It shows what all data we have in RDD
rdd.collect()

# It will give the first value in the RDD
# Getting some information from list
rdd.first()

# max() is going to arrange all the keys (1st element of the tuple) in alphabetical order
# and returns the max key and element
rdd.max()

# min() is going to arrange all the keys (1st element of the tuple) in alphabetical order
# and returns the min key and element
rdd.min()

rdd = spark.sparkContext.parallelize([1.0, 5.0, 43.0, 10.0])
rdd.max()

# It converts the numbers into string and Five is the first element
# This is a homework to explore why it is returning 5.0???????
rdd.max(key=str)

data = [('James','','Smith','1991-04-01','M',3000),
  ('Michael','Rose','','2000-05-19','M',4000),
  ('Robert','','Williams','1978-09-05','M',4000),
  ('Maria','Anne','Jones','1967-12-01','F',4000),
  ('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)

df.show()

columns = ["language","users_count"]
data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]

rdd = spark.sparkContext.parallelize(data)

# toDF() used to convert the rdd to dataframe
dfFromRDD1 = rdd.toDF()

dfFromRDD1.printSchema()

#Including columns to our dataframe
dfFromRDD1 = rdd.toDF(columns)

dfFromRDD1.printSchema()

dfFromRDD1.show()

from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)

pandasDF = df.toPandas()

pandasDF

pandasDF.shape

df.show()

df.show(truncate = False)

df.show(2,truncate=False)

df.show(2,truncate=10)

df.show()

columns = ["Seqno","Quote"]
data = [("1", "Be the change that you wish to see in the world"),
    ("2", "Everyone thinks of changing the world, but no one thinks of changing himself."),
    ("3", "The purpose of our.. lives is to be happy."),
    ("4", "Be cool.")]
df = spark.createDataFrame(data,columns)
df.show(truncate = False)

df.show(truncate = 25)

df.show(n=3,truncate=25,vertical=True)

df.show(truncate=25,vertical=True)

structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
df2.printSchema()

df2.show(truncate=False)

from pyspark.sql.functions import col,struct,when
updatedDF = df2.withColumn("OtherInfo", 
    struct(col("id").alias("identifier"),
    col("gender").alias("gender"),
    col("salary").alias("salary"),
    when(col("salary").cast(IntegerType()) < 2000,"Low")
      .when(col("salary").cast(IntegerType()) < 4000,"Medium")
      .otherwise("High").alias("Salary_Grade")
  )).drop("id","gender","salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)

